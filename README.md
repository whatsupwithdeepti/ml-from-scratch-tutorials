# üìò ML From Scratch Tutorials

A curated collection of Machine Learning tutorials created **from scratch**, combining theory, intuition, math, and hands-on implementation.  
These Jupyter notebooks walk through ML concepts step-by-step ‚Äî perfect for beginners, students, and anyone who wants to understand ML deeply rather than treating it as a black box.

Created by **Deepti Jethwani**.

---

## üìÇ Repository Contents (Detailed Explanation of Each File)

### **1. Key concepts of ML.ipynb**
This notebook introduces the fundamental concepts of Machine Learning:
- What ML is and how it works  
- Types of learning: Supervised, Unsupervised, Reinforcement  
- Key terms: features, labels, models, training, testing  
- Bias‚Äìvariance tradeoff  
- Underfitting vs Overfitting  
- Train/test split basics  
- Introduction to evaluation metrics  

**Goal:** To build a strong conceptual foundation before exploring algorithms.

---

### **2. MNIST - Classification Problem.ipynb**
This notebook walks through digit classification using the MNIST dataset:
- Loading and exploring the MNIST dataset  
- Preprocessing and normalization  
- Building a classification model  
- Training the model on 60,000 images  
- Testing on 10,000 images  
- Visualizing predictions and errors  
- Evaluating accuracy  

**Goal:** Learn end-to-end ML workflow on a real-world dataset.

---

### **3. Linear Regression ( Math ).ipynb**
A mathematical and practical overview of Linear Regression:
- Intuition of linear relationships  
- Hypothesis function  
- Cost function (MSE)  
- Gradient Descent explained step-by-step  
- Formula derivation  
- Implementing Linear Regression **from scratch**  
- Plotting regression lines and errors  

**Goal:** Understand the math and mechanics behind regression models.

---

### **4. Logistic Regression.ipynb**
A complete guide to Logistic Regression for classification:
- Logistic function and sigmoid  
- Why linear regression fails at classification  
- Decision boundary  
- Log loss / Cross-entropy  
- Gradient descent for logistic regression  
- Implementing logistic regression **from scratch**  
- Evaluating model accuracy  

**Goal:** Learn binary classification deeply.

---

### **5. SVM.ipynb**
Support Vector Machines explained with intuition and implementation:
- Maximum-margin concept  
- Hyperplanes and support vectors  
- Kernel trick (Linear, Polynomial, RBF)  
- Training an SVM classifier  
- Visualizing decision boundaries  
- Understanding when SVMs perform best  

**Goal:** Build intuition for one of the strongest classical ML models.

---

### **6. Decision Trees.ipynb**
A full tutorial on Decision Trees:
- Gini impurity  
- Entropy and information gain  
- How trees split data  
- Classification and regression trees  
- Implementing decision trees  
- Training on datasets  
- Tree visualization using `.dot` files  

**Goal:** Understand how tree-based models make decisions.

---

## üìÅ Repository Structure
ml-from-scratch-tutorials/
‚îÇ
‚îú‚îÄ‚îÄ 1. Key concepts of ML.ipynb
‚îú‚îÄ‚îÄ 2. MNIST - Classification Problem.ipynb
‚îú‚îÄ‚îÄ 3. Linear Regression ( Math ).ipynb
‚îú‚îÄ‚îÄ 4. Logistic Regression.ipynb
‚îú‚îÄ‚îÄ 5. SVM.ipynb
‚îú‚îÄ‚îÄ 6. Decision Trees.ipynb


##üì¢ Future Additions (Planned)
KNN from scratch
Naive Bayes
PCA
Random Forest
Gradient Boosting
Notes
